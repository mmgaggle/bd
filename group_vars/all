---
# Variables listed here are applicable to all host groups

# Can be 'bare' for bare metal, or 'ec2'
deploy_method: 'ec2'

# Set secrets via environmental variables
aws_access_key: "{{ lookup('env','AWS_ACCESS_KEY_ID') }}"
aws_secret_key: "{{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"

# Set NTP time server
ntpserver: time.nasa.gov

# S3A Configuration
# Use these to have Hadoop use a Ceph endpoint
s3a_endpoint: 's3.amazonaws.com'           # default 's3.amazonaws.com'
s3a_ssl_enabled: 'true'                    # default true
s3a_path_style_access: 'false'             # default false

# Defaults should be appropriate in Hadoop 2.8.1
s3a_connection_max: 15                     # default 15
s3a_attempts_max: 10                       # default 10
s3a_conn_establish_timeout: 5000           # default 5000
s3a_conn_timeout: 200000                   # default 200000
s3a_page_max: 5000                         # default 5000
s3a_threads_max: 10                        # default 10
s3a_socket_send_buffer: 8192               # default 8192
s3a_socket_recv_buffer: 8192               # default 8192
s3a_keepalive: 60                          # default 60
s3a_max_total_tasks: 5                     # default 5
s3a_multipart_size: '100M'                 # default 100M
s3a_multipart_threshold: 2147483647        # default 2147483647
s3a_multiobjectdelete_enable: 'true'       # default true
s3a_multipart_purge: 'false'               # default false
s3a_multipart_purge_age: 86400             # default 86400

# S3A defaults to AWSv4 signing in Hadoop 2.8.1
# When using a Ceph release before Jewel, set to 'S3SignerType'
# to force AWSv2 signing.
s3a_signing_algorithm: ''                  # default ''

s3a_buffer_dir: '${hadoop.tmp.dir}/s3a'    # default '${hadoop.tmp.dir}/s3a'
s3a_block_size: '32M'                      # default 32M
s3a_user_agent_prefix: ''                  # default ''
s3a_readahead_range: '64K'                 # default '64K'

# S3A with a disk buffer is pretty sane, so we turn it on
s3a_fast_upload: 'true'                    # default 'false'
s3a_fast_upload_buffer: 'disk'             # default 'disk'
s3a_fast_upload_active_blocks: 8           # default 8
s3a_input_fadvise: 'sequential'            # default 'sequential'

# Hadoop
hadoop_tmp_dir: '/tmp/hadoop-${user.name}' # default '/tmp/hadoop-${user.name}'

# YARN supports S3A buckets for log aggregation in Hadoop 2.8.1.
# This is useful if compute nodes have limited disk capacity.
yarn_nm_remote_app_log_dir: '/tmp/logs'    # default '/tmp/logs'

# Mappers will spill data here, so make sure you have enough capacity
# defaults to  '${hadoop.tmp.dir}/nm-local-dir'
yarn_nm_local_dirs: '${hadoop.tmp.dir}/nm-local-dir'

# HDFS data node directories (can be multiple, comma sep)
dfs_datanode_data_dir: '/hadoop/hdfs'

# How much memory to give the name node in GB
hdfs_namenode_memory: 32

# Spark
limits_nofile_hard: 65536
limits_nofile_soft: 65536

# Hive
mysql_port: 3306
hive_metastore_database_name: metastore
hive_metastore_mysql_user: hive
hive_metastore_mysql_password: password

hive_metastore_warehouse_dir: s3a://sa-summit-demo/
hive_scratchdir: s3a://sa-summit-demo/

# Presto
presto_staging_dir: '/tmp/presto'

# Secor
secor_bucket: 'stream'
secor_consumer_threads: 1
secor_max_filesize: 20000000
secor_max_fileage: 60
